{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [1.         1.         0.83333333 0.93333333 0.8       ]\n",
      "Average CV score:  0.9133333333333333\n",
      "Number of CV scores used in average:  5\n"
     ]
    }
   ],
   "source": [
    "# performing different methods on Iris dta to better understand CV\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "X,y=datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# methods to corss-validation:\n",
    "# 1. K-Fold: The training data used in the model is split, into k number of smaller sets, to be used to validate the model. The model is then trained on k-1 folds of training set. The remaining fold is then used as a validation set to evaluate the model.\n",
    "\n",
    "# create and fit model for validation\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# evaluating the mdoel to see its performance on each k-fold\n",
    "k_folds = KFold(n_splits=5)\n",
    "scores = cross_val_score(clf,X,y,cv=k_folds)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV score: \", scores.mean())\n",
    "print(\"Number of CV scores used in average: \", len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
      "Average CV score:  0.9533333333333334\n",
      "Number of CV scores used in average:  5\n"
     ]
    }
   ],
   "source": [
    "# Stratified K-Fold\n",
    "\"\"\"In cases where classes are imbalanced we need a way to account for the imbalance in both the train and validation sets. To do so we can stratify the target classes, meaning that both sets will have an equal proportion of all classes.\n",
    "\"\"\"\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "X,y = datasets.load_iris(return_X_y=True)\n",
    "cls = DecisionTreeClassifier(random_state=42)\n",
    "sk_folds = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(cls,X,y,cv=sk_folds)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV score: \", scores.mean())\n",
    "print(\"Number of CV scores used in average: \", len(scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the number of folds is the same, the average CV increases from the basic k-fold when making sure there is stratified classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
